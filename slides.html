<!DOCTYPE html>
<html>
  <head>
    <title>  Text Modeling</title>
    <meta charset="utf-8">
    <meta name="author" content=" Julia Silge | IBM Community Day: AI" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <script src="https://use.fontawesome.com/5235085b15.js"></script>
    <link rel="stylesheet" href="css/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="css/footer_plus.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




layout: true

&lt;div class="my-footer"&gt;&lt;span&gt;bit.ly/silge-ibm-ai-day&lt;/span&gt;&lt;/div&gt; 

---

background-image: url(figs/ibm_bumper_front.png)
background-size: cover

---

class: center, middle

background-image: url(figs/white_title.svg)
background-size: cover

&lt;img src="figs/so-logo.svg" width="30%"/&gt;

# Text Modeling

### USING TIDY DATA PRINCIPLES

### Julia Silge | IBM Community Day: AI

---
class: left, middle

background-image: url(figs/white_bg.svg)
background-size: cover

&lt;img src="figs/so-icon.svg" width="15%"/&gt;

## Find me at...

&lt;a href="http://twitter.com/juliasilge"&gt;&lt;i class="fa fa-twitter fa-fw"&gt;&lt;/i&gt;&amp;nbsp; @juliasilge&lt;/a&gt;&lt;br&gt;
&lt;a href="http://github.com/juliasilge"&gt;&lt;i class="fa fa-github fa-fw"&gt;&lt;/i&gt;&amp;nbsp; @juliasilge&lt;/a&gt;&lt;br&gt;
&lt;a href="https://juliasilge.com"&gt;&lt;i class="fa fa-link fa-fw"&gt;&lt;/i&gt;&amp;nbsp; juliasilge.com&lt;/a&gt;&lt;br&gt;
&lt;a href="https://tidytextmining.com"&gt;&lt;i class="fa fa-book fa-fw"&gt;&lt;/i&gt;&amp;nbsp; tidytextmining.com&lt;/a&gt;&lt;br&gt;
&lt;a href="mailto:julia.silge@gmail.com"&gt;&lt;i class="fa fa-paper-plane fa-fw"&gt;&lt;/i&gt;&amp;nbsp; julia.silge@gmail.com&lt;/a&gt;

---


background-image: url(figs/white_bg.svg)
background-size: cover

# Text in the real world

--

- .large[Text data is increasingly important üìö]

--

- .large[NLP training is scarce on the ground üò±]

---

background-image: url(figs/cant_even.gif)
background-position: 50% 50%
background-size: 700px

---

background-image: url(figs/vexing.gif)
background-position: 50% 50%
background-size: 650px

---

class: center, middle

background-image: url(figs/tidytext.png)
background-size: 500px

---

class: center, middle

background-image: url(figs/tidytext_repo.png)
background-size: 900px

---

class: center, middle

background-image: url(figs/cover.png)
background-size: 450px

---

background-image: url(figs/white_title.svg)
background-size: cover

# Two powerful NLP modeling approaches

--

- .large[Topic modeling]

--

- .large[Text classification]

---

background-image: url(figs/white_bg.svg)
background-size: cover

# Topic modeling

- .large[Each document = mixture of topics]

--

- .large[Each topic = mixture of words]

---

background-image: url(figs/top_tags-1.png)
background-size: 800px

---

class: center, middle

background-image: url(figs/white_title.svg)
background-size: cover

# GREAT LIBRARY HEIST üïµÔ∏è‚Äç‚ôÄÔ∏è

---

## Downloading your text data


```r
library(tidyverse)
library(gutenbergr)

titles &lt;- c("Twenty Thousand Leagues under the Sea", 
            "The War of the Worlds",
            "Pride and Prejudice", 
            "Great Expectations")

books &lt;- gutenberg_works(title %in% titles) %&gt;%
  gutenberg_download(meta_fields = "title")

books
```

```
## # A tibble: 51,663 x 3
##    gutenberg_id text                                       title          
##           &lt;int&gt; &lt;chr&gt;                                      &lt;chr&gt;          
##  1           36 The War of the Worlds                      The War of the‚Ä¶
##  2           36 ""                                         The War of the‚Ä¶
##  3           36 by H. G. Wells [1898]                      The War of the‚Ä¶
##  4           36 ""                                         The War of the‚Ä¶
##  5           36 ""                                         The War of the‚Ä¶
##  6           36 "     But who shall dwell in these worlds‚Ä¶ The War of the‚Ä¶
##  7           36 "     inhabited? .  .  .  Are we or they ‚Ä¶ The War of the‚Ä¶
##  8           36 "     World? .  .  .  And how are all thi‚Ä¶ The War of the‚Ä¶
##  9           36 "          KEPLER (quoted in The Anatomy ‚Ä¶ The War of the‚Ä¶
## 10           36 ""                                         The War of the‚Ä¶
## # ... with 51,653 more rows
```

---

## Someone has torn your books apart! üò≠



```r
by_chapter &lt;- books %&gt;%
  group_by(title) %&gt;%
  mutate(chapter = cumsum(str_detect(text, 
                                     regex("^chapter ", 
                                           ignore_case = TRUE)))) %&gt;%
  ungroup() %&gt;%
  filter(chapter &gt; 0) %&gt;%
  unite(document, title, chapter)

by_chapter
```

```
## # A tibble: 51,602 x 3
##    gutenberg_id text                                      document        
##           &lt;int&gt; &lt;chr&gt;                                     &lt;chr&gt;           
##  1           36 CHAPTER ONE                               The War of the ‚Ä¶
##  2           36 ""                                        The War of the ‚Ä¶
##  3           36 THE EVE OF THE WAR                        The War of the ‚Ä¶
##  4           36 ""                                        The War of the ‚Ä¶
##  5           36 ""                                        The War of the ‚Ä¶
##  6           36 No one would have believed in the last y‚Ä¶ The War of the ‚Ä¶
##  7           36 century that this world was being watche‚Ä¶ The War of the ‚Ä¶
##  8           36 intelligences greater than man's and yet‚Ä¶ The War of the ‚Ä¶
##  9           36 men busied themselves about their variou‚Ä¶ The War of the ‚Ä¶
## 10           36 scrutinised and studied, perhaps almost ‚Ä¶ The War of the ‚Ä¶
## # ... with 51,592 more rows
```

---

## Can we put them back together?


```r
library(tidytext)

word_counts &lt;- by_chapter %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(get_stopwords(source = "smart")) %&gt;%
  count(document, word, sort = TRUE)

word_counts
```

```
## # A tibble: 111,650 x 3
##    document               word        n
##    &lt;chr&gt;                  &lt;chr&gt;   &lt;int&gt;
##  1 Great Expectations_57  joe        88
##  2 Great Expectations_7   joe        70
##  3 Pride and Prejudice_18 mr         66
##  4 Great Expectations_17  biddy      63
##  5 Great Expectations_27  joe        58
##  6 Great Expectations_38  estella    58
##  7 Great Expectations_2   joe        56
##  8 Great Expectations_23  pocket     53
##  9 Great Expectations_15  joe        50
## 10 Great Expectations_18  joe        50
## # ... with 111,640 more rows
```

---

## Can we put them back together?


```r
words_sparse &lt;- word_counts %&gt;%
  cast_sparse(document, word, n)

class(words_sparse)
```

```
## [1] "dgCMatrix"
## attr(,"package")
## [1] "Matrix"
```

---

## Train a topic model

Use a sparse matrix or a `quanteda::dfm` object as input


```r
library(stm)

topic_model &lt;- stm(words_sparse, K = 4, 
                   verbose = FALSE, init.type = "Spectral")

summary(topic_model)
```

```
## A topic model with 4 topics, 193 documents and a 18360 word dictionary.
```

```
## Topic 1 Top Words:
##  	 Highest Prob: mr, elizabeth, mrs, darcy, bennet, miss, jane 
##  	 FREX: elizabeth, darcy, bennet, bingley, wickham, collins, lydia 
##  	 Lift: wickham, nephew, phillips, brighton, meryton, bourgh, mend 
##  	 Score: elizabeth, darcy, bennet, bingley, wickham, jane, lydia 
## Topic 2 Top Words:
##  	 Highest Prob: captain, nautilus, sea, nemo, ned, conseil, land 
##  	 FREX: nautilus, nemo, ned, conseil, canadian, ocean, seas 
##  	 Lift: vanikoro, indian, d'urville, reefs, scotia, shark's, solidification 
##  	 Score: nautilus, nemo, ned, conseil, canadian, ocean, captain 
## Topic 3 Top Words:
##  	 Highest Prob: mr, joe, miss, time, pip, looked, herbert 
##  	 FREX: joe, pip, herbert, wemmick, havisham, estella, biddy 
##  	 Lift: towel, giv, whimple, meantersay, jew, rot, barnard's 
##  	 Score: joe, wemmick, pip, jaggers, havisham, estella, herbert 
## Topic 4 Top Words:
##  	 Highest Prob: people, martians, man, time, black, men, night 
##  	 FREX: martians, martian, woking, mars, curate, pine, ulla 
##  	 Lift: martians, mars, curate, shepperton, henderson, hood, ripley 
##  	 Score: martians, martian, woking, cylinder, curate, ulla, pine
```

---

## Exploring the output of topic modeling

.large[Time for tidying!]


```r
chapter_topics &lt;- tidy(topic_model, matrix = "beta")

chapter_topics
```

```
## # A tibble: 73,440 x 3
##    topic term       beta
##    &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;
##  1     1 joe   8.69e-104
##  2     2 joe   3.03e-139
##  3     3 joe   1.21e-  2
##  4     4 joe   3.28e- 19
##  5     1 mr    1.90e-  2
##  6     2 mr    1.91e-  4
##  7     3 mr    1.22e-  2
##  8     4 mr    1.15e- 45
##  9     1 biddy 3.21e- 80
## 10     2 biddy 3.84e-149
## # ... with 73,430 more rows
```

---

## Exploring the output of topic modeling


```r
top_terms &lt;- chapter_topics %&gt;%
  group_by(topic) %&gt;%
  top_n(10, beta) %&gt;%
  ungroup() %&gt;%
  arrange(topic, -beta)

top_terms
```

```
## # A tibble: 40 x 3
##    topic term         beta
##    &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;
##  1     1 mr        0.0190 
##  2     1 elizabeth 0.0141 
##  3     1 mrs       0.00886
##  4     1 darcy     0.00881
##  5     1 bennet    0.00694
##  6     1 miss      0.00674
##  7     1 jane      0.00652
##  8     1 bingley   0.00607
##  9     1 time      0.00493
## 10     1 good      0.00480
## # ... with 30 more rows
```

---
## Exploring the output of topic modeling


```r
top_terms %&gt;%
  mutate(term = reorder(term, beta)) %&gt;%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

---

![](slides_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;

---

## How are documents classified?


```r
chapters_gamma &lt;- tidy(topic_model, matrix = "gamma",
                       document_names = rownames(words_sparse))

chapters_gamma
```

```
## # A tibble: 772 x 3
##    document               topic    gamma
##    &lt;chr&gt;                  &lt;int&gt;    &lt;dbl&gt;
##  1 Great Expectations_57      1 0.000792
##  2 Great Expectations_7       1 0.00340 
##  3 Pride and Prejudice_18     1 1.000   
##  4 Great Expectations_17      1 0.0480  
##  5 Great Expectations_27      1 0.000367
##  6 Great Expectations_38      1 0.00110 
##  7 Great Expectations_2       1 0.000531
##  8 Great Expectations_23      1 0.432   
##  9 Great Expectations_15      1 0.000565
## 10 Great Expectations_18      1 0.000277
## # ... with 762 more rows
```

---

## How are documents classified?


```r
chapters_parsed &lt;- chapters_gamma %&gt;%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE)

chapters_parsed
```

```
## # A tibble: 772 x 4
##    title               chapter topic    gamma
##    &lt;chr&gt;                 &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;
##  1 Great Expectations       57     1 0.000792
##  2 Great Expectations        7     1 0.00340 
##  3 Pride and Prejudice      18     1 1.000   
##  4 Great Expectations       17     1 0.0480  
##  5 Great Expectations       27     1 0.000367
##  6 Great Expectations       38     1 0.00110 
##  7 Great Expectations        2     1 0.000531
##  8 Great Expectations       23     1 0.432   
##  9 Great Expectations       15     1 0.000565
## 10 Great Expectations       18     1 0.000277
## # ... with 762 more rows
```

---

## How are documents classified?


```r
chapters_parsed %&gt;%
  mutate(title = reorder(title, gamma * topic)) %&gt;%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title)
```

---

![](slides_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

---

class: center, middle

background-image: url(figs/white_title.svg)
background-size: cover

# GOING FARTHER üöÄ

---

background-image: url(figs/white_bg.svg)
background-size: cover

## Tidying model output

### Which words in each document are assigned to which topics?

- .large[`augment()`]
- .large[Add information to each observation in the original data]

---

background-image: url(figs/stm_video.png)
background-size: 850px

---

## Using stm

- .large[Document-level covariates]


```r
topic_model &lt;- stm(words_sparse, K = 0, init.type = "Spectral",
                   prevalence = ~s(Year),
                   data = covariates,
                   verbose = FALSE)
```

- .large[Use functions for `semanticCoherence()`, `checkResiduals()`, `exclusivity()`, and more!]

- .large[Check out http://www.structuraltopicmodel.com/]

- .large[See [my recent blog post](https://juliasilge.com/blog/evaluating-stm/) for how to choose `K`, the number of topics]

---


background-image: url(figs/model_diagnostic-1.png)
background-position: 50% 50%
background-size: 950px

---

background-image: url(figs/white_title.svg)
background-size: cover

# Stemming? 

.large[Advice from [Schofield &amp; Mimno](https://mimno.infosci.cornell.edu/papers/schofield_tacl_2016.pdf)]

.large["Comparing Apples to Apple: The Effects of Stemmers on Topic Models"]

---

class: right, middle

&lt;h1 class="fa fa-quote-left fa-fw"&gt;&lt;/h1&gt;

&lt;h2&gt; Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability. &lt;/h2&gt;

&lt;h1 class="fa fa-quote-right fa-fw"&gt;&lt;/h1&gt;

---

class: right, middle

background-image: url(figs/white_title.svg)
background-size: cover


# Text classification
&lt;h1 class="fa fa-balance-scale fa-fw"&gt;&lt;/h1&gt;

---

## Downloading your text data


```r
library(tidyverse)
library(gutenbergr)

titles &lt;- c("The War of the Worlds",
            "Pride and Prejudice")

books &lt;- gutenberg_works(title %in% titles) %&gt;%
  gutenberg_download(meta_fields = "title") %&gt;%
  mutate(document = row_number())

books
```

```
## # A tibble: 19,504 x 4
##    gutenberg_id text                                title         document
##           &lt;int&gt; &lt;chr&gt;                               &lt;chr&gt;            &lt;int&gt;
##  1           36 The War of the Worlds               The War of t‚Ä¶        1
##  2           36 ""                                  The War of t‚Ä¶        2
##  3           36 by H. G. Wells [1898]               The War of t‚Ä¶        3
##  4           36 ""                                  The War of t‚Ä¶        4
##  5           36 ""                                  The War of t‚Ä¶        5
##  6           36 "     But who shall dwell in these‚Ä¶ The War of t‚Ä¶        6
##  7           36 "     inhabited? .  .  .  Are we o‚Ä¶ The War of t‚Ä¶        7
##  8           36 "     World? .  .  .  And how are ‚Ä¶ The War of t‚Ä¶        8
##  9           36 "          KEPLER (quoted in The A‚Ä¶ The War of t‚Ä¶        9
## 10           36 ""                                  The War of t‚Ä¶       10
## # ... with 19,494 more rows
```

---

## Making a tidy dataset

.large[Use this kind of data structure for EDA! üíÖ]


```r
library(tidytext)

tidy_books &lt;- books %&gt;%
  unnest_tokens(word, text) %&gt;%
  group_by(word) %&gt;%
  filter(n() &gt; 50) %&gt;%
  ungroup

tidy_books
```

```
## # A tibble: 132,474 x 4
##    gutenberg_id title                 document word 
##           &lt;int&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;
##  1           36 The War of the Worlds        1 the  
##  2           36 The War of the Worlds        1 of   
##  3           36 The War of the Worlds        1 the  
##  4           36 The War of the Worlds        3 by   
##  5           36 The War of the Worlds        6 but  
##  6           36 The War of the Worlds        6 who  
##  7           36 The War of the Worlds        6 shall
##  8           36 The War of the Worlds        6 in   
##  9           36 The War of the Worlds        6 these
## 10           36 The War of the Worlds        6 if   
## # ... with 132,464 more rows
```

---

## Cast to a sparse matrix

.large[And build a dataframe with a response variable]


```r
sparse_words &lt;- tidy_books %&gt;%
  count(document, word, sort = TRUE) %&gt;%
  cast_sparse(document, word, n)

books_joined &lt;- data_frame(document = as.integer(rownames(sparse_words))) %&gt;%
  left_join(books %&gt;%
              select(document, title))
```

---

## Train a glmnet model


```r
library(glmnet)
library(doMC)
registerDoMC(cores = 8)

is_jane &lt;- books_joined$title == "Pride and Prejudice"

model &lt;- cv.glmnet(sparse_words, is_jane, family = "binomial", 
                   parallel = TRUE, keep = TRUE)

model
```

```
## $lambda
##  [1] 1.419674e-01 1.293554e-01 1.178638e-01 1.073931e-01 9.785261e-02
##  [6] 8.915966e-02 8.123896e-02 7.402191e-02 6.744601e-02 6.145429e-02
## [11] 5.599486e-02 5.102043e-02 4.648792e-02 4.235806e-02 3.859509e-02
## [16] 3.516641e-02 3.204232e-02 2.919577e-02 2.660210e-02 2.423884e-02
## [21] 2.208553e-02 2.012351e-02 1.833579e-02 1.670689e-02 1.522270e-02
## [26] 1.387035e-02 1.263815e-02 1.151541e-02 1.049241e-02 9.560298e-03
## [31] 8.710987e-03 7.937127e-03 7.232015e-03 6.589542e-03 6.004146e-03
## [36] 5.470754e-03 4.984747e-03 4.541916e-03 4.138425e-03 3.770778e-03
## [41] 3.435793e-03 3.130566e-03 2.852456e-03 2.599051e-03 2.368159e-03
## [46] 2.157778e-03 1.966087e-03 1.791425e-03 1.632280e-03 1.487273e-03
## [51] 1.355147e-03 1.234760e-03 1.125067e-03 1.025119e-03 9.340506e-04
## [56] 8.510721e-04 7.754652e-04 7.065750e-04 6.438048e-04 5.866110e-04
## [61] 5.344981e-04 4.870148e-04 4.437497e-04 4.043282e-04 3.684088e-04
## [66] 3.356804e-04 3.058595e-04 2.786877e-04 2.539299e-04 2.313715e-04
## [71] 2.108171e-04 1.920887e-04 1.750240e-04 1.594754e-04 1.453080e-04
## [76] 1.323993e-04 1.206373e-04 1.099202e-04 1.001552e-04 9.125767e-05
## [81] 8.315059e-05 7.576372e-05 6.903308e-05 6.290037e-05 5.731248e-05
## [86] 5.222100e-05 4.758183e-05 4.335479e-05 3.950327e-05 3.599391e-05
## [91] 3.279631e-05 2.988277e-05 2.722807e-05 2.480920e-05
## 
## $cvm
##  [1] 1.2770246 1.2624354 1.2500421 1.2396871 1.2258937 1.2091374 1.1931277
##  [8] 1.1789140 1.1613377 1.1429008 1.1217140 1.0980526 1.0763915 1.0550954
## [15] 1.0333904 1.0118210 0.9902488 0.9673087 0.9443380 0.9215761 0.8983574
## [22] 0.8746780 0.8514636 0.8284042 0.8057874 0.7841222 0.7635302 0.7437638
## [29] 0.7246345 0.7063930 0.6891031 0.6728885 0.6579294 0.6442303 0.6317210
## [36] 0.6202974 0.6098516 0.6002297 0.5914487 0.5834584 0.5761686 0.5696039
## [43] 0.5637145 0.5585224 0.5539414 0.5498847 0.5463711 0.5433317 0.5407208
## [50] 0.5385009 0.5366415 0.5350895 0.5338095 0.5327657 0.5319356 0.5312827
## [57] 0.5307816 0.5304055 0.5301359 0.5299671 0.5298794 0.5298657 0.5299121
## [64] 0.5300040 0.5301292 0.5302856 0.5304692 0.5306730 0.5308892 0.5311158
## [71] 0.5313482 0.5315844 0.5318204 0.5320550 0.5322882 0.5325182 0.5327455
## [78] 0.5329680 0.5331864 0.5334013 0.5336118 0.5338184 0.5340207 0.5342185
## [85] 0.5344125 0.5346021 0.5347881 0.5349697 0.5351477 0.5353222 0.5354938
## [92] 0.5356552 0.5358092 0.5359766
## 
## $cvsd
##  [1] 0.003531867 0.003585037 0.003701819 0.003802743 0.004010137
##  [6] 0.004171220 0.004352915 0.004498450 0.004745542 0.004925974
## [11] 0.005205747 0.005221626 0.005267961 0.005290045 0.005247266
## [16] 0.005235163 0.005245294 0.005305709 0.005317193 0.005389697
## [21] 0.005442802 0.005414497 0.005353303 0.005288250 0.005215918
## [26] 0.005171078 0.005160939 0.005197867 0.005260196 0.005314366
## [31] 0.005330707 0.005328819 0.005314458 0.005314996 0.005329775
## [36] 0.005374958 0.005443825 0.005534324 0.005617028 0.005694225
## [41] 0.005779220 0.005849268 0.005920344 0.005997941 0.006071563
## [46] 0.006139136 0.006210873 0.006296256 0.006383498 0.006473336
## [51] 0.006556400 0.006633804 0.006713103 0.006787404 0.006859759
## [56] 0.006935759 0.007007930 0.007078035 0.007142221 0.007201489
## [61] 0.007258087 0.007311243 0.007362821 0.007411730 0.007456659
## [66] 0.007499295 0.007542169 0.007581584 0.007617505 0.007650511
## [71] 0.007680820 0.007708607 0.007734470 0.007758035 0.007779448
## [76] 0.007798904 0.007816578 0.007832769 0.007847275 0.007860173
## [81] 0.007871617 0.007881454 0.007889548 0.007896606 0.007902559
## [86] 0.007907601 0.007911882 0.007915436 0.007918376 0.007920356
## [91] 0.007921950 0.007921711 0.007925710 0.007923595
## 
## $cvup
##  [1] 1.2805564 1.2660204 1.2537439 1.2434899 1.2299038 1.2133087 1.1974806
##  [8] 1.1834125 1.1660833 1.1478267 1.1269198 1.1032742 1.0816594 1.0603855
## [15] 1.0386376 1.0170561 0.9954941 0.9726144 0.9496552 0.9269658 0.9038002
## [22] 0.8800925 0.8568169 0.8336924 0.8110033 0.7892932 0.7686911 0.7489617
## [29] 0.7298947 0.7117074 0.6944338 0.6782173 0.6632439 0.6495453 0.6370508
## [36] 0.6256724 0.6152954 0.6057640 0.5970657 0.5891526 0.5819479 0.5754531
## [43] 0.5696349 0.5645204 0.5600130 0.5560239 0.5525820 0.5496280 0.5471043
## [50] 0.5449742 0.5431979 0.5417233 0.5405226 0.5395531 0.5387953 0.5382185
## [57] 0.5377895 0.5374836 0.5372781 0.5371686 0.5371375 0.5371770 0.5372749
## [64] 0.5374157 0.5375859 0.5377849 0.5380113 0.5382546 0.5385067 0.5387663
## [71] 0.5390290 0.5392930 0.5395548 0.5398130 0.5400677 0.5403171 0.5405621
## [78] 0.5408008 0.5410336 0.5412615 0.5414834 0.5416998 0.5419102 0.5421151
## [85] 0.5423151 0.5425097 0.5427000 0.5428852 0.5430661 0.5432425 0.5434157
## [92] 0.5435769 0.5437349 0.5439002
## 
## $cvlo
##  [1] 1.2734927 1.2588504 1.2463403 1.2358844 1.2218835 1.2049662 1.1887748
##  [8] 1.1744156 1.1565922 1.1379748 1.1165083 1.0928310 1.0711235 1.0498054
## [15] 1.0281431 1.0065858 0.9850035 0.9620030 0.9390208 0.9161864 0.8929146
## [22] 0.8692635 0.8461102 0.8231159 0.8005715 0.7789511 0.7583692 0.7385660
## [29] 0.7193743 0.7010787 0.6837724 0.6675596 0.6526150 0.6389153 0.6263912
## [36] 0.6149225 0.6044078 0.5946953 0.5858317 0.5777641 0.5703894 0.5637546
## [43] 0.5577942 0.5525245 0.5478698 0.5437456 0.5401603 0.5370354 0.5343373
## [50] 0.5320275 0.5300851 0.5284557 0.5270964 0.5259782 0.5250758 0.5243470
## [57] 0.5237737 0.5233275 0.5229937 0.5227656 0.5226214 0.5225545 0.5225493
## [64] 0.5225923 0.5226726 0.5227863 0.5229270 0.5230915 0.5232717 0.5234652
## [71] 0.5236674 0.5238758 0.5240859 0.5242969 0.5245088 0.5247193 0.5249290
## [78] 0.5251353 0.5253391 0.5255411 0.5257402 0.5259369 0.5261312 0.5263219
## [85] 0.5265099 0.5266945 0.5268762 0.5270543 0.5272293 0.5274018 0.5275718
## [92] 0.5277335 0.5278835 0.5280530
## 
## $nzero
##  s0  s1  s2  s3  s4  s5  s6  s7  s8  s9 s10 s11 s12 s13 s14 s15 s16 s17 
##   0   1   1   1   2   3   3   3   5   5  10  11  12  15  16  21  27  36 
## s18 s19 s20 s21 s22 s23 s24 s25 s26 s27 s28 s29 s30 s31 s32 s33 s34 s35 
##  42  52  67  79  92 106 121 130 143 161 177 191 203 211 223 232 244 262 
## s36 s37 s38 s39 s40 s41 s42 s43 s44 s45 s46 s47 s48 s49 s50 s51 s52 s53 
## 272 286 294 301 311 316 321 325 329 334 336 340 343 349 356 361 367 372 
## s54 s55 s56 s57 s58 s59 s60 s61 s62 s63 s64 s65 s66 s67 s68 s69 s70 s71 
## 374 376 376 380 380 381 382 385 388 390 391 393 394 397 401 401 401 403 
## s72 s73 s74 s75 s76 s77 s78 s79 s80 s81 s82 s83 s84 s85 s86 s87 s88 s89 
## 403 404 405 405 405 406 406 406 407 407 407 407 407 407 407 407 407 407 
## s90 s91 s92 s93 
## 407 408 408 408 
## 
## $name
##            deviance 
## "Binomial Deviance" 
## 
## $glmnet.fit
## 
## Call:  glmnet(x = sparse_words, y = is_jane, parallel = TRUE, family = "binomial") 
## 
##        Df       %Dev    Lambda
##  [1,]   0 -1.460e-14 1.420e-01
##  [2,]   1  1.187e-02 1.294e-01
##  [3,]   1  2.157e-02 1.179e-01
##  [4,]   1  2.955e-02 1.074e-01
##  [5,]   2  4.036e-02 9.785e-02
##  [6,]   3  5.358e-02 8.916e-02
##  [7,]   3  6.615e-02 8.124e-02
##  [8,]   3  7.714e-02 7.402e-02
##  [9,]   5  9.103e-02 6.745e-02
## [10,]   5  1.053e-01 6.145e-02
## [11,]  10  1.220e-01 5.599e-02
## [12,]  11  1.409e-01 5.102e-02
## [13,]  12  1.578e-01 4.649e-02
## [14,]  15  1.746e-01 4.236e-02
## [15,]  16  1.916e-01 3.860e-02
## [16,]  21  2.087e-01 3.517e-02
## [17,]  27  2.258e-01 3.204e-02
## [18,]  36  2.442e-01 2.920e-02
## [19,]  42  2.625e-01 2.660e-02
## [20,]  52  2.809e-01 2.424e-02
## [21,]  67  2.998e-01 2.209e-02
## [22,]  79  3.191e-01 2.012e-02
## [23,]  92  3.380e-01 1.834e-02
## [24,] 106  3.567e-01 1.671e-02
## [25,] 121  3.750e-01 1.522e-02
## [26,] 130  3.926e-01 1.387e-02
## [27,] 143  4.093e-01 1.264e-02
## [28,] 161  4.255e-01 1.152e-02
## [29,] 177  4.413e-01 1.049e-02
## [30,] 191  4.565e-01 9.560e-03
## [31,] 203  4.708e-01 8.711e-03
## [32,] 211  4.842e-01 7.937e-03
## [33,] 223  4.965e-01 7.232e-03
## [34,] 232  5.081e-01 6.590e-03
## [35,] 244  5.188e-01 6.004e-03
## [36,] 262  5.287e-01 5.471e-03
## [37,] 272  5.380e-01 4.985e-03
## [38,] 286  5.466e-01 4.542e-03
## [39,] 294  5.545e-01 4.138e-03
## [40,] 301  5.617e-01 3.771e-03
## [41,] 311  5.683e-01 3.436e-03
## [42,] 316  5.744e-01 3.131e-03
## [43,] 321  5.799e-01 2.852e-03
## [44,] 325  5.848e-01 2.599e-03
## [45,] 329  5.893e-01 2.368e-03
## [46,] 334  5.933e-01 2.158e-03
## [47,] 336  5.970e-01 1.966e-03
## [48,] 340  6.002e-01 1.791e-03
## [49,] 343  6.031e-01 1.632e-03
## [50,] 349  6.057e-01 1.487e-03
## [51,] 356  6.081e-01 1.355e-03
## [52,] 361  6.102e-01 1.235e-03
## [53,] 367  6.121e-01 1.125e-03
## [54,] 372  6.138e-01 1.025e-03
## [55,] 374  6.153e-01 9.341e-04
## [56,] 376  6.166e-01 8.511e-04
## [57,] 376  6.178e-01 7.755e-04
## [58,] 380  6.188e-01 7.066e-04
## [59,] 380  6.198e-01 6.438e-04
## [60,] 381  6.206e-01 5.866e-04
## [61,] 382  6.213e-01 5.345e-04
## [62,] 385  6.219e-01 4.870e-04
## [63,] 388  6.225e-01 4.437e-04
## [64,] 390  6.230e-01 4.043e-04
## [65,] 391  6.234e-01 3.684e-04
## [66,] 393  6.238e-01 3.357e-04
## [67,] 394  6.242e-01 3.059e-04
## [68,] 397  6.245e-01 2.787e-04
## [69,] 401  6.247e-01 2.539e-04
## [70,] 401  6.250e-01 2.314e-04
## [71,] 401  6.252e-01 2.108e-04
## [72,] 403  6.253e-01 1.921e-04
## [73,] 403  6.255e-01 1.750e-04
## [74,] 404  6.257e-01 1.595e-04
## [75,] 405  6.258e-01 1.453e-04
## [76,] 405  6.259e-01 1.324e-04
## [77,] 405  6.260e-01 1.206e-04
## [78,] 406  6.261e-01 1.099e-04
## [79,] 406  6.262e-01 1.002e-04
## [80,] 406  6.262e-01 9.126e-05
## [81,] 407  6.263e-01 8.315e-05
## [82,] 407  6.263e-01 7.576e-05
## [83,] 407  6.264e-01 6.903e-05
## [84,] 407  6.264e-01 6.290e-05
## [85,] 407  6.265e-01 5.731e-05
## [86,] 407  6.265e-01 5.222e-05
## [87,] 407  6.265e-01 4.758e-05
## [88,] 407  6.266e-01 4.335e-05
## [89,] 407  6.266e-01 3.950e-05
## [90,] 407  6.266e-01 3.599e-05
## [91,] 407  6.266e-01 3.280e-05
## [92,] 408  6.267e-01 2.988e-05
## [93,] 408  6.267e-01 2.723e-05
## [94,] 408  6.267e-01 2.481e-05
## 
## $fit.preval
##               [,1]      [,2]      [,3]      [,4]      [,5]      [,6]
##     [1,] 0.6623386 0.5890997 0.5175775 0.4524515 0.3965520 0.3493900
##     [2,] 0.6640334 0.6734436 0.6819385 0.6894440 0.6939526 0.6956162
##     [3,] 0.6623386 0.5890997 0.5175775 0.4524515 0.3965520 0.3493900
##     [4,] 0.6622538 0.6553561 0.6492951 0.6438633 0.6370363 0.6423742
##     [5,] 0.6628508 0.6072679 0.5526587 0.5023576 0.4576816 0.4182908
##     [6,] 0.6623386 0.6556310 0.6495300 0.6441157 0.6373997 0.6292901
##     [7,] 0.6623386 0.6061494 0.5515655 0.5013100 0.4566428 0.4172682
##     [8,] 0.6639442 0.6078942 0.5532546 0.5029204 0.4581318 0.4185843
##     [9,] 0.6625480 0.6115975 0.5570699 0.5067712 0.4620698 0.4224222
##    [10,] 0.6645958 0.6080139 0.5534866 0.5032551 0.4585211 0.4191130
##               [,7]      [,8]      [,9]     [,10]     [,11]     [,12]
##     [1,] 0.3089559 0.2742120 0.2469766 0.2238496 0.2046322 0.1895891
##     [2,] 0.6963260 0.6967588 0.6947223 0.6907024 0.6774487 0.6272760
##     [3,] 0.3089559 0.2742120 0.2469766 0.2238496 0.2046322 0.1895891
##     [4,] 0.6560244 0.6694991 0.6818825 0.6945405 0.7504289 0.8024264
##     [5,] 0.3832410 0.3520655 0.3258773 0.3028704 0.2823604 0.2663707
##     [6,] 0.6213098 0.6138925 0.6045996 0.5952989 0.5827722 0.5478395
##     [7,] 0.3822578 0.3511265 0.3252090 0.3023673 0.2819820 0.2656016
##     [8,] 0.3833776 0.3520593 0.3261219 0.3033389 0.2831412 0.2666856
##     [9,] 0.3871040 0.3557437 0.3296194 0.3066508 0.2863566 0.2700372
##    [10,] 0.3840542 0.3528619 0.3267154 0.3037917 0.2832302 0.2670871
##              [,13]     [,14]     [,15]     [,16]     [,17]      [,18]
##     [1,] 0.1765513 0.1656232 0.1573011 0.1515728 0.1463257 0.14352761
##     [2,] 0.5801526 0.5372517 0.5011082 0.4682341 0.4370333 0.40883409
##     [3,] 0.1765513 0.1656232 0.1548772 0.1464883 0.1391657 0.13461343
##     [4,] 0.8440785 0.8763205 0.9012044 0.9204444 0.9314626 0.93874677
##     [5,] 0.2522686 0.2398954 0.2299397 0.2233294 0.2174419 0.21417470
##     [6,] 0.5041857 0.4654456 0.4331159 0.4056881 0.3796373 0.35661897
##     [7,] 0.2512778 0.2387923 0.2290491 0.2039872 0.1743193 0.15215132
##     [8,] 0.2523312 0.2399777 0.2304467 0.2236070 0.2176679 0.21340653
##     [9,] 0.2555545 0.2431404 0.2325154 0.2245141 0.2191983 0.21558921
##    [10,] 0.2528281 0.2403314 0.2284066 0.2174213 0.1971663 0.17669587
##               [,19]      [,20]      [,21]      [,22]      [,23]      [,24]
##     [1,] 0.14205330 0.14111876 0.14086072 0.14059892 0.13975709 0.13887555
##     [2,] 0.38047873 0.35380912 0.33040915 0.31069239 0.29427356 0.27944996
##     [3,] 0.13166347 0.12950129 0.12820176 0.12703765 0.12553115 0.12412950
##     [4,] 0.94462714 0.94936990 0.95303111 0.95612228 0.95875427 0.96113043
##     [5,] 0.21219909 0.21035544 0.21024111 0.21028873 0.20937606 0.20734954
##     [6,] 0.33459569 0.31417541 0.29641781 0.28133084 0.26795519 0.25554070
##     [7,] 0.13501964 0.12104444 0.10196014 0.08352781 0.06806688 0.05615145
##     [8,] 0.21157506 0.21033609 0.21003414 0.20931169 0.20812331 0.19612422
##     [9,] 0.21277073 0.22257869 0.23970553 0.24283200 0.24407250 0.24437258
##    [10,] 0.15997349 0.14517503 0.12514742 0.10849658 0.09552879 0.08474795
##               [,25]      [,26]       [,27]       [,28]       [,29]
##     [1,] 0.13772264 0.13593582 0.133579364 0.130887664 0.128092045
##     [2,] 0.26540624 0.25204012 0.238996824 0.227094040 0.217604833
##     [3,] 0.12259955 0.12059405 0.118182763 0.115474019 0.112690217
##     [4,] 0.96306873 0.96447905 0.965707602 0.966436428 0.966845875
##     [5,] 0.20435894 0.20134366 0.198302062 0.195245029 0.192068513
##     [6,] 0.24374603 0.23169802 0.220379355 0.210430313 0.201444736
##     [7,] 0.04670631 0.03907367 0.032581951 0.026627847 0.021916726
##     [8,] 0.18380101 0.17201990 0.160617356 0.149896552 0.139238685
##     [9,] 0.24469371 0.24537103 0.246503948 0.247643710 0.249190564
##    [10,] 0.07537087 0.06696450 0.059268185 0.052188479 0.046077637
##                [,30]       [,31]       [,32]       [,33]       [,34]
##     [1,] 0.129946457 0.131574880 0.132828188 0.133609654 0.134389235
##     [2,] 0.209161744 0.201658914 0.194135174 0.184208103 0.174921573
##     [3,] 0.110063218 0.107345540 0.104713129 0.101980325 0.099329971
##     [4,] 0.967050051 0.967435764 0.968140936 0.970279009 0.972203361
##     [5,] 0.188855034 0.184899806 0.179599032 0.174470100 0.169883022
##     [6,] 0.192997253 0.184697792 0.177044496 0.164986961 0.153641516
##     [7,] 0.018134532 0.015007708 0.012453655 0.010366618 0.008678623
##     [8,] 0.124638672 0.111874213 0.100605927 0.090617370 0.081837718
##     [9,] 0.248503131 0.244674867 0.240213904 0.235885216 0.232242996
##    [10,] 0.040571742 0.035523065 0.030953932 0.026893847 0.023301344
##                [,35]       [,36]        [,37]        [,38]        [,39]
##     [1,] 0.134946275 0.135114420 0.1363603728 0.1401725513 0.1425700039
##     [2,] 0.165709058 0.157485848 0.1497055116 0.1418867720 0.1347225003
##     [3,] 0.096694404 0.093871572 0.0911498777 0.0882749406 0.0854916460
##     [4,] 0.973683714 0.975055047 0.9763579749 0.9775670611 0.9787081761
##     [5,] 0.165489770 0.161216607 0.1569849979 0.1525731974 0.1481568490
##     [6,] 0.143447034 0.134375066 0.1258879621 0.1192537420 0.1136208316
##     [7,] 0.007296763 0.006144660 0.0051893468 0.0043752303 0.0037022192
##     [8,] 0.074057831 0.067599056 0.0621426368 0.0572336369 0.0526799299
##     [9,] 0.228708347 0.224922715 0.2208741182 0.2172430205 0.2133754826
##    [10,] 0.019828213 0.016743042 0.0141169182 0.0119083056 0.0100438731
##                 [,40]        [,41]        [,42]        [,43]        [,44]
##     [1,] 0.1442836697 0.1459696754 0.1479317454 1.500511e-01 1.518822e-01
##     [2,] 0.1285299303 0.1235136845 0.1187713036 1.141108e-01 1.096401e-01
##     [3,] 0.0827224777 0.0804843654 0.0786030276 7.678796e-02 7.478957e-02
##     [4,] 0.9796863807 0.9805734093 0.9813742058 9.816922e-01 9.818818e-01
##     [5,] 0.1438091120 0.1391712446 0.1351015502 1.315380e-01 1.260900e-01
##     [6,] 0.1091039725 0.1044562177 0.0998508076 9.551832e-02 9.153953e-02
##     [7,] 0.0031504506 0.0026881873 0.0023011654 1.973163e-03 1.697596e-03
##     [8,] 0.0484206849 0.0444702375 0.0409058572 3.761542e-02 3.453980e-02
##     [9,] 0.2095715258 0.2059368324 0.2024323984 1.990074e-01 1.955259e-01
##    [10,] 0.0084499846 0.0070990560 0.0059563556 4.984450e-03 4.165167e-03
##                 [,45]        [,46]        [,47]        [,48]        [,49]
##     [1,] 1.536658e-01 1.552257e-01 1.566964e-01 1.582195e-01 1.595892e-01
##     [2,] 1.055709e-01 1.018644e-01 9.831658e-02 9.510241e-02 9.195524e-02
##     [3,] 7.282168e-02 7.086502e-02 6.903639e-02 6.731694e-02 6.566671e-02
##     [4,] 9.824444e-01 9.833291e-01 9.841772e-01 9.849719e-01 9.857082e-01
##     [5,] 1.196002e-01 1.134537e-01 1.077894e-01 1.030435e-01 9.851392e-02
##     [6,] 8.783313e-02 8.456258e-02 8.150395e-02 7.859945e-02 7.415473e-02
##     [7,] 1.465418e-03 1.268595e-03 1.102126e-03 9.628228e-04 8.467336e-04
##     [8,] 3.168686e-02 2.896639e-02 2.645958e-02 2.422593e-02 2.224260e-02
##     [9,] 1.919411e-01 1.881810e-01 1.843255e-01 1.804380e-01 1.768715e-01
##    [10,] 3.478700e-03 2.905930e-03 2.425592e-03 2.028845e-03 1.698470e-03
##                 [,50]        [,51]        [,52]        [,53]        [,54]
##     [1,] 1.608984e-01 1.621370e-01 1.633782e-01 1.646379e-01 1.658838e-01
##     [2,] 8.912367e-02 8.638593e-02 8.382953e-02 8.142187e-02 7.921649e-02
##     [3,] 6.405384e-02 6.251800e-02 6.105653e-02 5.965187e-02 5.833983e-02
##     [4,] 9.863548e-01 9.869575e-01 9.875038e-01 9.880249e-01 9.885026e-01
##     [5,] 9.415746e-02 9.007938e-02 8.627594e-02 8.264775e-02 7.934652e-02
##     [6,] 6.947140e-02 6.524760e-02 6.143379e-02 5.796442e-02 5.486367e-02
##     [7,] 7.477033e-04 6.635065e-04 5.919820e-04 5.307925e-04 4.781535e-04
##     [8,] 2.047829e-02 1.887223e-02 1.742720e-02 1.613036e-02 1.497846e-02
##     [9,] 1.734095e-01 1.699848e-01 1.666238e-01 1.633152e-01 1.601481e-01
##    [10,] 1.421767e-03 1.190329e-03 9.969094e-04 8.362543e-04 7.030084e-04
##                 [,55]        [,56]        [,57]        [,58]        [,59]
##     [1,] 1.671105e-01 1.682983e-01 1.693567e-01 1.703109e-01 1.712046e-01
##     [2,] 7.718471e-02 7.531044e-02 7.359133e-02 7.202118e-02 7.057762e-02
##     [3,] 5.712914e-02 5.599903e-02 5.496684e-02 5.399387e-02 5.307974e-02
##     [4,] 9.889520e-01 9.893707e-01 9.897570e-01 9.901157e-01 9.904458e-01
##     [5,] 7.631158e-02 7.350469e-02 7.060367e-02 6.737469e-02 6.443886e-02
##     [6,] 5.209148e-02 4.960210e-02 4.737145e-02 4.535527e-02 4.352371e-02
##     [7,] 4.330111e-04 3.940856e-04 3.604576e-04 3.310092e-04 3.052036e-04
##     [8,] 1.394967e-02 1.302161e-02 1.218614e-02 1.143292e-02 1.076052e-02
##     [9,] 1.571631e-01 1.543194e-01 1.516182e-01 1.490841e-01 1.467172e-01
##    [10,] 5.916848e-04 4.986373e-04 4.210089e-04 3.582697e-04 3.073316e-04
##                 [,60]        [,61]        [,62]        [,63]        [,64]
##     [1,] 1.721105e-01 1.729821e-01 1.738475e-01 1.746858e-01 1.755281e-01
##     [2,] 6.924570e-02 6.798443e-02 6.680754e-02 6.571071e-02 6.470835e-02
##     [3,] 5.222563e-02 5.142202e-02 5.068860e-02 5.000124e-02 4.936928e-02
##     [4,] 9.907514e-01 9.910269e-01 9.912765e-01 9.915065e-01 9.917180e-01
##     [5,] 6.177311e-02 5.935541e-02 5.718860e-02 5.526265e-02 5.354899e-02
##     [6,] 4.186453e-02 4.036302e-02 3.900450e-02 3.769911e-02 3.652490e-02
##     [7,] 2.827161e-04 2.631482e-04 2.460666e-04 2.312423e-04 2.183881e-04
##     [8,] 1.016020e-02 9.624650e-03 9.146508e-03 8.722502e-03 8.347877e-03
##     [9,] 1.444331e-01 1.422467e-01 1.401655e-01 1.381789e-01 1.363268e-01
##    [10,] 2.640917e-04 2.272275e-04 1.958465e-04 1.691711e-04 1.464503e-04
##                 [,65]        [,66]        [,67]        [,68]        [,69]
##     [1,] 1.763251e-01 1.770907e-01 1.778103e-01 1.784978e-01 1.791598e-01
##     [2,] 6.379357e-02 6.295467e-02 6.216780e-02 6.144081e-02 6.075778e-02
##     [3,] 4.877897e-02 4.824084e-02 4.774271e-02 4.728648e-02 4.686539e-02
##     [4,] 9.919154e-01 9.920973e-01 9.922627e-01 9.924132e-01 9.925501e-01
##     [5,] 5.200982e-02 5.062208e-02 4.936882e-02 4.823416e-02 4.721026e-02
##     [6,] 3.546372e-02 3.451623e-02 3.365418e-02 3.287931e-02 3.217812e-02
##     [7,] 2.070174e-04 1.969035e-04 1.879303e-04 1.799803e-04 1.729176e-04
##     [8,] 8.012384e-03 7.711917e-03 7.441792e-03 7.199163e-03 6.980804e-03
##     [9,] 1.345556e-01 1.328749e-01 1.313004e-01 1.298488e-01 1.284649e-01
##    [10,] 1.270688e-04 1.105133e-04 9.634478e-05 8.418708e-05 7.374126e-05
##                 [,70]        [,71]        [,72]        [,73]        [,74]
##     [1,] 1.797697e-01 1.803239e-01 1.808480e-01 1.813362e-01 1.817972e-01
##     [2,] 6.013447e-02 5.956215e-02 5.904177e-02 5.856251e-02 5.812708e-02
##     [3,] 4.647872e-02 4.611969e-02 4.579244e-02 4.549069e-02 4.521618e-02
##     [4,] 9.926745e-01 9.927887e-01 9.928929e-01 9.929880e-01 9.930744e-01
##     [5,] 4.628018e-02 4.544303e-02 4.468053e-02 4.398833e-02 4.335577e-02
##     [6,] 3.154426e-02 3.096903e-02 3.044971e-02 2.997882e-02 2.955381e-02
##     [7,] 1.666320e-04 1.610180e-04 1.560067e-04 1.515118e-04 1.474854e-04
##     [8,] 6.784482e-03 6.607211e-03 6.447312e-03 6.302687e-03 6.172003e-03
##     [9,] 1.271674e-01 1.259574e-01 1.246995e-01 1.234662e-01 1.223070e-01
##    [10,] 6.474007e-05 5.697335e-05 5.024402e-05 4.440973e-05 3.932718e-05
##                 [,75]        [,76]        [,77]        [,78]        [,79]
##     [1,] 1.822240e-01 1.826179e-01 1.829887e-01 1.833306e-01 1.836551e-01
##     [2,] 5.772926e-02 5.736381e-02 5.703279e-02 5.673037e-02 5.645161e-02
##     [3,] 4.496429e-02 4.473150e-02 4.451976e-02 4.432460e-02 4.414802e-02
##     [4,] 9.931533e-01 9.932259e-01 9.932921e-01 9.933529e-01 9.934089e-01
##     [5,] 4.278387e-02 4.226499e-02 4.179024e-02 4.135962e-02 4.096783e-02
##     [6,] 2.916884e-02 2.881790e-02 2.850008e-02 2.821036e-02 2.798306e-02
##     [7,] 1.438664e-04 1.406075e-04 1.376788e-04 1.350369e-04 1.326508e-04
##     [8,] 6.053688e-03 5.946731e-03 5.849916e-03 5.762058e-03 5.682348e-03
##     [9,] 1.212392e-01 1.202498e-01 1.193452e-01 1.185159e-01 1.177472e-01
##    [10,] 3.489165e-05 3.101909e-05 2.761767e-05 2.462701e-05 2.199925e-05
##                 [,80]        [,81]        [,82]        [,83]        [,84]
##     [1,] 1.839581e-01 1.842358e-01 1.844953e-01 1.847331e-01 1.849550e-01
##     [2,] 5.619664e-02 5.596314e-02 5.574781e-02 5.555168e-02 5.537015e-02
##     [3,] 4.398706e-02 4.383864e-02 4.370384e-02 4.357982e-02 4.346737e-02
##     [4,] 9.934598e-01 9.935063e-01 9.935494e-01 9.935885e-01 9.936242e-01
##     [5,] 4.061332e-02 4.029161e-02 3.999770e-02 3.973154e-02 3.948850e-02
##     [6,] 2.778319e-02 2.760081e-02 2.743527e-02 2.728416e-02 2.714704e-02
##     [7,] 1.304970e-04 1.285488e-04 1.267900e-04 1.251973e-04 1.237580e-04
##     [8,] 5.610096e-03 5.544470e-03 5.484904e-03 5.430856e-03 5.381740e-03
##     [9,] 1.170369e-01 1.163811e-01 1.157737e-01 1.152166e-01 1.147056e-01
##    [10,] 1.967309e-05 1.761173e-05 1.578838e-05 1.416274e-05 1.272045e-05
##                 [,85]        [,86]        [,87]        [,88]        [,89]
##     [1,] 1.851577e-01 1.853466e-01 1.855187e-01 1.856787e-01 1.858265e-01
##     [2,] 5.520512e-02 5.505394e-02 5.491656e-02 5.479133e-02 5.467651e-02
##     [3,] 4.336394e-02 4.327023e-02 4.318403e-02 4.310597e-02 4.303496e-02
##     [4,] 9.936569e-01 9.936865e-01 9.937136e-01 9.937383e-01 9.937607e-01
##     [5,] 3.926843e-02 3.906741e-02 3.888521e-02 3.871929e-02 3.856844e-02
##     [6,] 2.702186e-02 2.690828e-02 2.680455e-02 2.671042e-02 2.662485e-02
##     [7,] 1.224530e-04 1.212725e-04 1.202009e-04 1.192305e-04 1.183508e-04
##     [8,] 5.337154e-03 5.296619e-03 5.259807e-03 5.226321e-03 5.195897e-03
##     [9,] 1.142332e-01 1.137989e-01 1.134009e-01 1.130354e-01 1.126966e-01
##    [10,] 1.142828e-05 1.027060e-05 1.000000e-05 1.000000e-05 1.000000e-05
##                 [,90]        [,91]        [,92]        [,93]        [,94]
##     [1,] 1.859626e-01 1.860875e-01 1.862211e-01 1.863084e-01 1.864184e-01
##     [2,] 5.457288e-02 5.447812e-02 5.439799e-02 5.431351e-02 5.424687e-02
##     [3,] 4.297025e-02 4.291121e-02 4.286308e-02 4.280855e-02 4.276834e-02
##     [4,] 9.937812e-01 9.938006e-01 9.938190e-01 9.938326e-01 9.938498e-01
##     [5,] 3.843025e-02 3.830580e-02 3.819151e-02 3.808926e-02 3.799498e-02
##     [6,] 2.654698e-02 2.647606e-02 2.641622e-02 2.635306e-02 2.630347e-02
##     [7,] 1.175526e-04 1.168279e-04 1.161881e-04 1.155715e-04 1.150464e-04
##     [8,] 5.168230e-03 5.143063e-03 5.119622e-03 5.099133e-03 5.079787e-03
##     [9,] 1.123880e-01 1.121155e-01 1.118502e-01 1.116190e-01 1.113959e-01
##    [10,] 1.000000e-05 1.000000e-05 1.000000e-05 1.000000e-05 1.000000e-05
##  [ reached getOption("max.print") -- omitted 15907 rows ]
## 
## $foldid
##    [1]  3  1  3  4  8  3  3  5  7 10  2  3  6  9  3  9  7 10  8  8  5  3  7
##   [24]  9  8  1  7 10  2  7  2  1  2  7  7  7  6  3  1  8  2  7  3  5  4  9
##   [47]  4  9  3  9  8  4  9  5  6  4  8  5  9  3  2  1  7  4  9  8  6  1  6
##   [70]  3  5  8  4  2  3  1  1  2  2 10  9  7  2  1  9  5  6  4  8  7  6  5
##   [93]  8 10  3  9  7 10  3  8 10  7  9  7  1  8  7  2  9  7 10  9  6  2  6
##  [116]  9  4  2  3  3  3  8  9 10  4  2  5  9  3 10  9  9  6 10  3  1  9 10
##  [139]  8  6  2  8  3 10  5  3  1  2  3  6  1  3  8  3  5  6  4  9  5  3  3
##  [162]  9  7  5  5  7 10  7  5  2  2  1  8  7  2  7  6 10  7  5  3  1  4 10
##  [185]  2  7  3  4  2  7  8  8  7  3 10  7 10  5  7  2  6 10  8  1  5  4  7
##  [208]  2  7  4  1  8  9  7  8  1  8  7  2  6  2  5  6 10  9  2  4  5  6  4
##  [231] 10  6  1  3  8  9  7  3  4  2  4  2  5  3  8  7  4  4  7  6  9  5  7
##  [254]  6  9  1  2  6  5  1  1  5  3  9  1  2  5  5  2  6  5  1  8  6  3  1
##  [277]  8  4  1  4  8  8  7  1  9  1  8  3  1 10 10  6  3  3  5  4  4  3  3
##  [300]  7  2  5  4  7  6  9 10  5 10  3  2  9  6  6  7  6  6  9  7  6  1  7
##  [323]  1  1  3  2  1  8  5  5  2  5  5  7  9  7  7  9  6  4  5  6  4  2  1
##  [346] 10  8  4  7  5  8  9  7  8  8  7  4  2  2  8  6  9  2  9  4  5  4  1
##  [369]  8  7  2 10  6  6  2 10  9  1  4  4  4 10  3  2  4  1  2  7  6  3  2
##  [392]  1  5  1  6  1  1  2  5  2  8  2  9  8  3  8  6  3  9  8  3  1  8  5
##  [415]  9  3  6 10  9  9  8  1  8 10  8  8 10  3  4  2  4  8  6  1  8  6  8
##  [438]  3  9  1  1  2 10  9  4  7  4  5  4  9  1  5  8  4  1  2  4  3  5  1
##  [461] 10  6  7  4  3  4  2  7  6  4  6  1  3  4  9  8  8  4  8 10  9  9  1
##  [484]  9  1  3  2  1  6  5  3  8  3  6  9  6  8  2  4  5  5  9  5  5  1  9
##  [507]  4 10  8  2  6  8  7  5  5  5  1  2  9  2  8  6  8  9  1  1  7  9  3
##  [530]  9  9  3  4  7  4  2 10  1  8  9  6  1  8  1  2  7 10  9  7  6  5  2
##  [553]  4  2  6  2  9 10  3 10  8 10  9  1 10  4  4  4  7  6  2  6  8  3  7
##  [576]  7  2 10  1  2  7  5  5  5 10  8  7  6 10 10  9  3  8 10  9 10  4  2
##  [599]  3  9  3  4  6  6  5  5 10  4  6  7  8  9  2  8  1  7  6  8  1  6  4
##  [622]  8  1  8  8  1  8  2  8  3  1  2  4  3  1  1  6 10  2 10 10  6  8  7
##  [645]  9  8 10 10  8  5  8  5 10  1  4  7  9  2  8  4  3  1  3  3 10  8  8
##  [668]  8 10  2  9  1  2  5 10  4  4  3  8  6  5  2  5  7  8  8 10  7  9  9
##  [691]  3  4  3  5  8  3 10  6  4  6  3  3  8  7  2  5  7  9  4  3  2  3  4
##  [714]  8  2  2  3  2  3  7  5  3  2  8  1  7  9  2 10  9  2  4  3  1  9  5
##  [737]  8  3  5  7  1  6  1  7  8 10  5  8  3  6  1  3  9  5  5  2  8  4  4
##  [760]  5  9  1  3  1  5  1  8  9  3  4  9  2  5  3  3  4  9  1  6  6  1 10
##  [783]  8  6  9  7 10  2 10  3  3  7  1  9  4  1  1  2  5  2  2  9  9 10  5
##  [806]  5  6  5  7  2  4  5  6 10  7  8  9  5 10  5  9  1  2  9  6  7  8  8
##  [829]  3  3  9  6  1  8  9  5  1  1 10  8 10  9  1  2  7  4  8  9 10  2  7
##  [852]  3  6  1  3  6  6  2  8  9  4  2  7  6  1  2  9  2  9  7  8  3 10  8
##  [875]  4  7  9  6  7  6  8  5  7  1  1  3  3  8  4  4  5  6  2  3  7  6  8
##  [898] 10  3  3  7  5  6  7 10  6  8 10  9  6  4  7  7  2  7  5  7  6  2  9
##  [921]  5  6 10  2  5  8  2  1  6  3  6  4  3  3  4  8  8  7  6  5  5  8  4
##  [944]  4  2  2  1  7  8  6  8  3  5  6  8  5  5  4  6 10  5  1  7  9 10  4
##  [967] 10  6  8  4  4  6  3 10  8  4  1  5  2  4  6  9  8  4  9  7  6 10  5
##  [990]  7  9  2  6  8  1 10  1  3  9  4
##  [ reached getOption("max.print") -- omitted 14917 entries ]
## 
## $lambda.min
## [1] 0.0004870148
## 
## $lambda.1se
## [1] 0.001355147
## 
## attr(,"class")
## [1] "cv.glmnet"
```

---

## Tidying our model

.large[Tidy, then filter to choose some lambda from glmnet output]


```r
library(broom)

coefs &lt;- model$glmnet.fit %&gt;%
  tidy() %&gt;%
  filter(lambda == model$lambda.1se)

Intercept &lt;- coefs %&gt;%
  filter(term == "(Intercept)") %&gt;%
  pull(estimate)
```

---

## Tidying our model


```r
classifications &lt;- tidy_books %&gt;%
  inner_join(coefs, by = c("word" = "term")) %&gt;%
  group_by(document) %&gt;%
  summarize(Score = sum(estimate)) %&gt;%
  mutate(Probability = plogis(Intercept + Score))

classifications
```

```
## # A tibble: 15,896 x 3
##    document  Score Probability
##       &lt;int&gt;  &lt;dbl&gt;       &lt;dbl&gt;
##  1        1 -0.928     0.267  
##  2        3  0.158     0.520  
##  3        6  2.44      0.914  
##  4        7 -0.837     0.286  
##  5        8 -0.634     0.329  
##  6        9 -0.410     0.380  
##  7       13 -0.230     0.423  
##  8       15 -5.23      0.00494
##  9       19  0.549     0.615  
## 10       21 -0.928     0.267  
## # ... with 15,886 more rows
```

---

## Understanding our model


```r
coefs %&gt;%
  group_by(estimate &gt; 0) %&gt;%
  top_n(10, abs(estimate)) %&gt;%
  ungroup %&gt;%
  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate &gt; 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip()
```

---

![](slides_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;

---

## ROC


```r
comment_classes &lt;- classifications %&gt;%
  left_join(books %&gt;%
              select(title, document), by = "document") %&gt;%
  mutate(Correct = case_when(title == "Pride and Prejudice" ~ TRUE,
                             TRUE ~ FALSE))

roc &lt;- comment_classes %&gt;%
  arrange(desc(Probability)) %&gt;%
  mutate(TPR = cumsum(Correct) / sum(Correct),
         FPR = cumsum(!Correct) / sum(!Correct),
         FDR = cummean(!Correct))
```

---

## ROC


```r
roc %&gt;%
  arrange(Probability)
```

```
## # A tibble: 15,896 x 8
##    document  Score Probability title             Correct   TPR   FPR   FDR
##       &lt;int&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;             &lt;lgl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1     2570 -11.4    0.0000103 The War of the W‚Ä¶ FALSE       1 1     0.337
##  2      716 -10.4    0.0000272 The War of the W‚Ä¶ FALSE       1 1.000 0.337
##  3     2103  -9.85   0.0000489 The War of the W‚Ä¶ FALSE       1 1.000 0.337
##  4     5358  -9.65   0.0000594 The War of the W‚Ä¶ FALSE       1 0.999 0.337
##  5     2094  -9.55   0.0000655 The War of the W‚Ä¶ FALSE       1 0.999 0.336
##  6     1374  -9.18   0.0000949 The War of the W‚Ä¶ FALSE       1 0.999 0.336
##  7      791  -9.04   0.000109  The War of the W‚Ä¶ FALSE       1 0.999 0.336
##  8      559  -8.73   0.000149  The War of the W‚Ä¶ FALSE       1 0.999 0.336
##  9     3450  -8.72   0.000150  The War of the W‚Ä¶ FALSE       1 0.999 0.336
## 10     1360  -8.72   0.000151  The War of the W‚Ä¶ FALSE       1 0.998 0.336
## # ... with 15,886 more rows
```

---

![](slides_files/figure-html/unnamed-chunk-25-1.png)&lt;!-- --&gt;

---

## AUC for model


```r
roc %&gt;%
  summarise(AUC = sum(diff(FPR) * na.omit(lead(TPR) + TPR)) / 2)
```

```
## # A tibble: 1 x 1
##     AUC
##   &lt;dbl&gt;
## 1 0.959
```

---

## Misclassifications

Let's talk about misclassifications. Which documents here were incorrectly predicted to be written by Jane Austen?


```r
roc %&gt;%
  filter(Probability &gt; .8, !Correct) %&gt;%
  sample_n(10) %&gt;%
  inner_join(books %&gt;%
               select(document, text)) %&gt;%
  select(Probability, text)
```

```
## # A tibble: 10 x 2
##    Probability text                                                       
##          &lt;dbl&gt; &lt;chr&gt;                                                      
##  1       0.986 should be prepared.  It seems to me that it should be poss‚Ä¶
##  2       0.973 wheels flung behind her, she receded with terrifying slown‚Ä¶
##  3       0.809 "\"Good Lord!\" said Henderson.  \"Fallen meteorite!  That‚Ä¶
##  4       0.892 had my doubts.  You're slender.  I didn't know that it was‚Ä¶
##  5       0.803 ill-tempered.                                              
##  6       0.956 certain further details which, although they were not all ‚Ä¶
##  7       0.839 not consider it necessary to guard it, and a chance of esc‚Ä¶
##  8       0.965 with a violent thud, a blinding flash, her decks, her funn‚Ä¶
##  9       0.810 even to continue watching.  Let it suffice to say, blood o‚Ä¶
## 10       0.807 luckily the dull radiation arrested him before he could bu‚Ä¶
```

---

## Misclassifications

Let's talk about misclassifications. Which documents here were incorrectly predicted to *not* be written by Jane Austen?


```r
roc %&gt;%
  filter(Probability &lt; .2, Correct) %&gt;%
  sample_n(10) %&gt;%
  inner_join(books %&gt;%
               select(document, text)) %&gt;%
  select(Probability, text)
```

```
## # A tibble: 10 x 2
##    Probability text                                                       
##          &lt;dbl&gt; &lt;chr&gt;                                                      
##  1      0.184  rapturous air, the fine proportion and the finished orname‚Ä¶
##  2      0.196  immediately wandering up in the street in quest of the off‚Ä¶
##  3      0.183  imagine that their silence was to last through the two dan‚Ä¶
##  4      0.197  Lydia's voice was heard in the vestibule; the door was thr‚Ä¶
##  5      0.123  a beautiful wood stretching over a wide extent.            
##  6      0.111  in earnest. I speak nothing but the truth. He still loves ‚Ä¶
##  7      0.0914 The wisest and the best of men--nay, the wisest and best o‚Ä¶
##  8      0.146  it seems but a fortnight I declare; and yet there have bee‚Ä¶
##  9      0.0998 My conscience told me that I deserved no extraordinary pol‚Ä¶
## 10      0.0694 occasional appearance of some trout in the water, and talk‚Ä¶
```

---


background-image: url(figs/tmwr_0601.png)
background-position: 50% 70%
background-size: 750px

## Workflow for text mining/modeling

---
background-image: url(figs/lizzieskipping.gif)
background-position: 50% 55%
background-size: 750px

# Go explore real-world text!

---

class: left, middle

background-image: url(figs/white_bg.svg)
background-size: cover

&lt;img src="figs/so-icon.svg" width="15%"/&gt;

# Thanks!

&lt;a href="http://twitter.com/juliasilge"&gt;&lt;i class="fa fa-twitter fa-fw"&gt;&lt;/i&gt;&amp;nbsp; @juliasilge&lt;/a&gt;&lt;br&gt;
&lt;a href="http://github.com/juliasilge"&gt;&lt;i class="fa fa-github fa-fw"&gt;&lt;/i&gt;&amp;nbsp; @juliasilge&lt;/a&gt;&lt;br&gt;
&lt;a href="https://juliasilge.com"&gt;&lt;i class="fa fa-link fa-fw"&gt;&lt;/i&gt;&amp;nbsp; juliasilge.com&lt;/a&gt;&lt;br&gt;
&lt;a href="https://tidytextmining.com"&gt;&lt;i class="fa fa-book fa-fw"&gt;&lt;/i&gt;&amp;nbsp; tidytextmining.com&lt;/a&gt;&lt;br&gt;
&lt;a href="mailto:julia.silge@gmail.com"&gt;&lt;i class="fa fa-paper-plane fa-fw"&gt;&lt;/i&gt;&amp;nbsp; julia.silge@gmail.com&lt;/a&gt;

Slides created with [remark.js](http://remarkjs.com/) and the R package [**xaringan**](https://github.com/yihui/xaringan)

---

background-image: url(figs/ibm_bumper_end.png)
background-size: cover
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
